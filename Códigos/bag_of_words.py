# -*- coding: utf-8 -*-
"""bag-of-words.ipynb

Automatically generated by Colaboratory.

"""

import re
import numpy 
from sklearn.feature_extraction.text import CountVectorizer 
from sklearn.tree import DecisionTreeClassifier as dt
# import a library to split the original dataset into random test and train subsets
from sklearn.model_selection import train_test_split
# libraries to evaluate the model
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
from sklearn.svm import NuSVC
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import TfidfVectorizer

def word_extraction(sentence):
	ignore = ['a', "the", "is"]
	words = re.sub("[^\w]", " ",  sentence).split()
	cleaned_text = [w.lower() for w in words if w not in ignore]
	return cleaned_text

def tokenize(sentences):
	words = []
	for sentence in sentences:
		w = word_extraction(sentence)
		words.extend(w)
		words = sorted(list(set(words)))
	return words

def generate_bow(allsentences):
	vocab = tokenize(allsentences)
	print("Word List for Document \n{0} \n".format(vocab));
	for sentence in allsentences:
		words = word_extraction(sentence)
		bag_vector = numpy.zeros(len(vocab))
		for w in words:
			for i,word in enumerate(vocab):
				if word == w:
					bag_vector[i] += 1
		print("{0}\n{1}\n".format(sentence,numpy.array(bag_vector)))

import nltk 
from nltk.corpus import stopwords 
nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

"""# BoW"""

allsentences = ["Joe waited for the train", "The train was late", "Mary and Samantha took the bus", "I looked for Mary and Samantha at the bus station","Mary and Samantha arrived at the bus station early but waited until noon for the bus"]
generate_bow(allsentences)
print("Com a biblioteca:")
vectorizer = CountVectorizer()
X = vectorizer.fit_transform(allsentences)
target = numpy.array([1,1,2,2,2])
print(X.toarray())

print("Treinando e testando com o mesmo conjunto de dados")

X_train,X_test,y_train,y_test = train_test_split(X, target, test_size=0.3, random_state=4)
print(X_train.toarray())
print(X_test.toarray())
mySVC = SVC()

# train
mySVC.fit(X_train, y_train)

preds = mySVC.predict(X_test)
# print(preds)


acc = accuracy_score(y_test, preds)
print(acc)


print("############################")

my_vocabulary = vectorizer.vocabulary_

newsentences = ["The train is very fast","The bus is full"]
newvectorizer = CountVectorizer(vocabulary=my_vocabulary, stop_words=stop_words)

X = newvectorizer.fit_transform(newsentences)
new_target = numpy.array([1,2])
preds2 = mySVC.predict(X)
acc = accuracy_score(new_target, preds2)
print("Testando com novo conjunto de dados")
print(acc)

"""#TF IDF"""

vec = TfidfVectorizer()
X = vec.fit_transform(allsentences)
X_train,X_test,y_train,y_test = train_test_split(X, target, test_size=0.3, random_state=4)
newSVC = SVC()

# train
newSVC.fit(X_train, y_train)

preds = mySVC.predict(X_test)

print(X_train.toarray())
print(X_test.toarray())
print(preds)
acc = accuracy_score(y_test, preds)
print(acc)
